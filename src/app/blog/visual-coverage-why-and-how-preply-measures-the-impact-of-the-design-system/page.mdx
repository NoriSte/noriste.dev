# Visual coverage: Why and How Preply Measures the Impact of the Design System (Part 1/2)

*This article has been originally posted [on Preply's engineering blog](https://medium.com/preply-engineering/visual-coverage-why-and-how-preply-measures-the-impact-of-the-design-system-1057115f4aff).*

We implemented a custom way to measure the impact of the Design System, and we measure it on user‚Äôs devices. Why? Why were the existing solutions and approaches not enough? Why is it so crucial for us that everyone, management included, can understand and visualize the design system‚Äôs impact?

We worked for months to come up with our own way to measure the design system impact, and we shared all the whys and the hows in this article. A longer, technical one ([The Implementation Details of Preply‚Äôs Design System Visual Coverage](https://dev.to/noriste/the-implementation-details-of-preplys-design-system-visual-coverage-part-ii-1ao2)), explains what we did. Last but not least, the code is open source ([https://github.com/preply/design-system-visual-coverage](https://github.com/preply/design-system-visual-coverage)).

Why Did We Need a Custom Way to Track the Impact of the Design System?
======================================================================

Preply is a data-driven company. All the teams elaborate on a long-term strategy (following [the Good Strategy, Bad Strategy](https://medium.com/mbreads/good-strategy-bad-strategy-b37b2563a4a3) book approach) and define the metrics that allow them to measure their successes toward that strategy. The metrics are evaluated, challenged, discussed, tweaked, and approved. The design system team makes no exceptions.

We found that none of the existing ways to track the impact of design systems could tell the actual influence Path (Preply‚Äôs Design System) has on Preply‚Äôs users. **Explaining static analysis data (tokens and components used in the codebase) to the CEO is hard**. We needed to measure the impact on what the users see and interact with.

Since 2021, static analysis has shown a positive trend regarding the design system‚Äôs adoption, reporting a boost during Preply‚Äôs 2023 re-brand. However, this data expresses the influence on the codebase, not the users.

![A graph showing the growing adoption of the design system in the Preply's front-end projects. At the end of 2023, it shows that 25% of the front-end codebases come from the design system.](https://miro.medium.com/v2/resize:fit:2000/format:webp/0*CJri5mgDbKcWtaTf)
*A screenshot of the internal design system stats we collected. The labels at the top left are the internal project names.*

What Have We Implemented? The Design System Visual Coverage
===========================================================

The idea is:

1.  The borders of the DOM elements are colored green for those generated by Path design system components and red for the others.
2.  The borders‚Äô size depends on the importance of the components.
3.  The formula is green pixels / (green pixels + red pixels) (the final formula slightly changed, but this is the core concept). So, **100% means that ‚Äúall the DOM elements of a page come from Path Design System components.‚Äù**

Here is a screenshot of the visual coverage on Preply.com‚Äôs home page.

![The Preply.com page showing the design system visual coverage in action: every DOM element has a colored border.](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-WcFFM9M0KrWTf8Q)
*Here, you can see the visual coverage in action with borders colored and weighted based on the type of components.*

The visual coverage tracks how much the Path Design System components are used in Preply‚Äôs products.

How Is the Visual Coverage Used?
================================

The visual coverage serves different purposes:

The Design System team can:

1.  Constantly track the outcomes of its initiatives. Please note that visual coverage is not the only metric we use.
2.  Identify where Path Design System components are used the most and the least, and work with Product teams to increase the visual coverage.

The product teams have:

1.  Numbers they can use as OKRs.
2.  Straightforward tools to gather the visual coverage in seconds straight where they work: in production, locally, or in Storybook.
3.  Straightforward tools to split the ownership of the pages across different teams. Most of Preply.com‚Äôs pages are made of components that belong to various teams.

To serve the mentioned purposes, we built an internal dashboard.

![The dashboard shows the current visual coverage, the average per team, the trends, etc.](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*L1VV-0TBRje4QglrCwUWwA.png)
*Our dashboard with teams, pages, and overall visual coverage.*

Product Teams‚Äô Ownership and Coverage Containers
------------------------------------------------

On Preply.com, some pages are owned by a single team, whereas different teams own others. Visual coverage containers are areas of the page that are analyzed and counted independently.

![The screenshot shows how a single page can be split in different containers owned by different teams.](https://miro.medium.com/v2/resize:fit:2000/format:webp/0*Dj_AeIWUnRfDQKE_)
*The coverage containers in action: in the screenshot, there are three different containers: the Header, the Calendar, and the rest of the page. A different Preply team owns each of them.*

While speaking with teams, we have always stressed that **there is no good or bad visual coverage**. In Preply, Path Design System is heavily used, and when it‚Äôs not used very much, there are strong reasons. For instance:

1.  Calendars: Only one team uses calendars, and Path Design System does not offer pre-made calendar components. That team has low coverage compared to the others, and that‚Äôs fine!
2.  A payment button that comes from a third-party integration. It looks like a Preply button, but it is not.
3.  Every custom component has unique needs that the design system components do not offer.

At the same time, all the newly implemented product parts or the frequently refactored parts have the highest score, which has grown organically.

This topic is essential because we never ask teams to reach a specific visual coverage score. **Visual coverage is a litmus** test for many other factors heavily influenced by the Design System team and initiatives, not a goal to achieve at any cost.

Where to Run the Visual Coverage Count? On Users‚Äô Devices!
----------------------------------------------------------

The implementation of visual coverage (we shared an ‚Äúimplementation details‚Äù article [here](https://dev.to/noriste/the-implementation-details-of-preplys-design-system-visual-coverage-part-ii-1ao2)) ensures that it doesn‚Äôt affect the user‚Äôs UX. That‚Äôs why we can track visual coverage in production on users‚Äô devices. This choice made everything more complex from a technical perspective (we also a/b tested it to ensure UX was not affected). Still, it also made the design system team independent in implementing, deploying, evolving, and gathering thousands of data daily.

If we decided to embed the visual coverage inside Preply‚Äôs E2E tests, we would have needed product teams to help us. However, product teams have product priorities, and depending on them would have meant delaying the project.

Some considerations for measuring the visual coverage on users‚Äô devices:

1.  Users use different devices with varying **screen ratios and sizes**, making using a fixed device as a reference impossible.
2.  Users see different things based on the **user-generated content** and custom interactions with the Product.
3.  The more a page is visited, the more calculations it sends. The final visual coverage score is an average of all the calculations.

As a result, the visual coverage is not 100% stable and can vary daily for the same page. We embraced this variability because the coverage would reflect what our users see!

Pages‚Äô popularity impact
------------------------

Again, the more a page is visited, the more calculations it sends. I want to stress this topic because it‚Äôs crucial to visual coverage. When we count the visual coverage, we send two pieces of data to the server: the number of pixels of the Path Design System components‚Äô perimeters and the number of pixels of the other DOM elements. In DataDog (used for our dashboard), we calculate the average.

Imagine this scenario (numbers are imaginary):

**The Home page is visited 5 times**, and it sends the following events:

1.  One event for the Design System pixels count. Let‚Äôs imagine the value is 10K
2.  One event Non-Design System pixels: 5K

In total, the Home page sent 10 events. The Home page‚Äôs visual coverage is 67%.

**The Setting page is visited 1 times**, and it sends the following events:

1.  One event for the Design System pixels count. Let‚Äôs imagine the value is 1K
2.  One event Non-Design System pixels: 20K

In total, the Settings page sent 4 events. The Settings page‚Äôs visual coverage is 4%.

The math done in the dashboard is: all the Design System pixels count / divided by all the pixel counts. (10K*5 + 1K*2) / (10K*5 + 1K*2 + 5K*5 + 20K*2) = 44%. Can you see how **the high popularity of the Home page weighs more** than the low popularity of the Settings page? The final 44% is closer to the Home page‚Äôs 67% than the Settings page‚Äôs 4%.

Now, consider that our public and indexed pages are visited millions of times, and other ‚Äúprivate‚Äù pages are visited hundreds of thousands of times. Other B2B pages could have ‚Äújust‚Äù tens of thousands of visits. This is crucial information because, when we do the math, the teams behind the B2B pages **could be entirely obfuscated by the popularity of other teams‚Äô public pages**. That‚Äôs why the Design System team cares more about the average per team than the final number (at the time of writing, our visual coverage is 77%, but most of the teams are above 85%) because the per-team visual coverage allows more granular monitoring.

The Components‚Äô Weights
-----------------------

What counts for the visual coverage are the component‚Äôs perimeters. The perimeters can‚Äôt be the same for all the components for some reasons:

1.  Path Design System components are UI components, while the **pages contain hundreds of layout containers**, which don‚Äôt impact the UX. UI components are smaller than containers, so the latter weigh more if the weight is the same.
2.  A Design System‚Äôs divider doesn‚Äôt impact the UX as much as a searchable dropdown. At the same time, a Dropdown **saves developers weeks of work** while a Divider saves seconds.

So, the component weights are crucial for the visual coverage. We had a workshop to define the component‚Äôs importance. The idea is

1.  To identify a 0‚Äì5 score for the component‚Äôs impact in terms of **UI, UX, and accessibility**.
2.  We did it empirically to identify the importance of every factor: every team member voted and gave a 1‚Äì3 score to every factor; the importance is the sum of the votes.
3.  Then, multiply the scores we identified at point 1 for the importance identified at point 2.
4.  **The result is a relative component‚Äôs score**. In our case, it goes from 0 to 60.

![The spreadsheet with all the components and all the scores.](https://miro.medium.com/v2/resize:fit:2000/format:webp/0*G8X1bKdhp7AcanS1)
*The spreadsheet we used to identify the weight to assign to every component.*

The actual weight we use for the visual coverage is reduced by a factor we identified through multiple iterations. For each iteration, we analyzed the score of the most representative pages (some of them are the most important ones, and some have low Path Design System‚Äôs components usage), **empirically looking for the right coverage for the pages**.

Other Design System Metrics
===========================

While elaborating on a long-term Design System strategy and a yearly roadmap, we thought of which meaningful metrics we could use to track the Path Design System‚Äôs success. The visual coverage seems to group all of them (because an increase in the visual coverage is the litmus of many other initiatives). Still, at the same time, it can‚Äôt reflect everything the design system team does. The metrics we care about are:

1.  The Design System **NPS score**, of course.
2.  **Designers‚Äô and developers‚Äô efficiency**: initially, this is a soft metric we ask through surveys without a real process in place to calculate it
3.  The percentage of Path Design System **foundations and components documented**.
4.  Path Design System **platform features parity**: the three platforms are Figma, Web, and App. Path Design System should support all of them at the same level (considering some unique capabilities and needs. For instance, Figma does not require Path‚Äôs components to be SSR/RSC-friendly).

And, more in general, we monitor

1.  Path‚Äôs components usage: static analysis is insightful for the design system team.
2.  Figma‚Äôs components detachments through Figma Analytics.

We also discarded implementing some metrics:

1.  **Figma adoption**: We differentiate coverage (usage) from adoption (**correct** usage), but after some experiments and chats with other Design System teams, it seems too early for us to work on it, given the time required to implement and maintain something reliable.
2.  **The amount of CSS introduced per PR:** This is one of the metrics Cristiano Rastelli identified in his [Measuring the Impact of a Design System](https://didoo.medium.com/measuring-the-impact-of-a-design-system-7f925af090f7), which is the best resource on this topic. We suggest reading the article to find links to other companies‚Äô activities to track their Design System impact.

Implementation details
======================

Implementing the visual coverage, collecting feedback, improving it, validating it through a company-wide visual coverage increase initiative, etc., required months of work. Other companies we discussed showed interest, too.

If you want to experiment with it, we've got you covered:

1.  Read (or send it to your engineers) [the second part of this article](https://dev.to/noriste/the-implementation-details-of-preplys-design-system-visual-coverage-part-ii-1ao2), which is dedicated to the implementation details.
2.  Fork, tweak, and use [the visual coverage code](https://github.com/preply/design-system-visual-coverage).

Let us know what you think, and contact us at design-system [AT] preply.com, or directly with me on [X](https://x.com/NoriSte), [GitHub](https://github.com/NoriSte), or [LinkedIn](https://www.linkedin.com/in/noriste/).

Resources
=========

Preply looks very different from September 2023, and Path Design System is now quite connected to the new brand. If you are curious about how Preply executed a company-wide revolution in a few months, please read [Maryna](https://www.linkedin.com/in/maryna-shananina-a8535a82/)‚Äôs dedicated article, [Turning the Page: Preply‚Äôs Successful Journey Through the Rebrand](https://medium.com/preply-engineering/turning-the-page-preplys-successful-journey-through-the-rebrand-e1ebf2a4612f)).

Special Thanks
==============

I want to publicly kudos all the people involved in this project:

1.  The whole design system team: [Justine](http://justine), [Seif](https://www.linkedin.com/in/seifkamal/), [Alex](https://www.linkedin.com/in/lightespresso/), [Javi](https://www.linkedin.com/in/javierarques/), [Alisa](https://www.linkedin.com/in/alisa-antypova/). I love this team üòç
2.  [Vadym](https://www.linkedin.com/in/vadym-vlasenko-699a9222/), [Igor](https://www.linkedin.com/in/ihor-kasianov-b58b783a/), [Josh](https://www.linkedin.com/in/joshcrossick/), [Dmytro](https://www.linkedin.com/in/dmytrovoloshyn/), [Dominika](https://www.linkedin.com/in/dominika-szuka%C5%82a-kosi%C5%84ska-1240627a/), for the support and feedback üòä
3.  [Oleg](https://www.linkedin.com/in/oleg-buiar/), for creating the dashboard and helping with all the data.
4.  [Daniel](https://www.linkedin.com/in/daniel-guerra-guerrero-a64180a9/), [Alberto](https://www.linkedin.com/in/albertovilva), for the early internal feedback.
5.  All the Preply front-end engineers for supporting the design system, and this initiative. All the Preply app engineers for helping with the React Native implementation.
6.  [Javier](https://www.linkedin.com/in/jalvarez88/), [Serhii](https://www.linkedin.com/in/serhii-tanskyi-243779143/), [Volodymyr](https://www.linkedin.com/in/matsola/), for supporting me with the React Native implementation.
7.  [Matteo](https://www.linkedin.com/in/matteoronchi/), for trying the visual coverage with a different company.
8.  [Massimiliano](https://www.linkedin.com/in/massimilianomantione/), for the performance suggestions.
9.  [Dmitry](https://www.linkedin.com/in/blv-dmitry/), [Omri](https://www.linkedin.com/in/omri-lavi/), for the feedback.
10.  [Nicolas](https://x.com/beaussan), for the frigging detailed review ‚ù§Ô∏è.

_Would you like to join me and work in a purpose-driven organization where work, growth, and learning happen at the same time? Preply continues growing and we are actively looking for talented candidates to join our Engineering team! If you are excited about taking on a new challenge,_ [_check out our open positions here_](https://bit.ly/3Ei3FRt)_._
